# parl-
强化学习入门

通过为期一周的强化学习，利用parl框架，实现了对强化学习的基础算法实战。通过科科老师的耐心讲解，掌握了一些基本的算法理论以及各种算法的适应性。（鄙人初次接触强化学习，如有不对，请批评指正！）
下面介绍一下，一周学习的主要涉及到的内容：
    学习平台：百度AIStudio
    Python模块: paddle、parl、gym、atari-py、rlschool、numpy
    学习策略：Sarsa、QLearning、DQN、PG、DDPG

课节1: 强化学习(RL)初印象
      1.课程基础+编程基础+机器学习基础
      2.什么是强化学习
      3.Agent学习的两种方案：基于价值与基于策略
      4.RL概览分类
      5.算法库&框架库
      6.PARL介绍：
总结：1.parl的代码可读性好，函数功能清晰
      2.支持算法数量种类多，可复性好
      3.模块之间耦合度低，内聚性强
      4.大规模分布式能力，只需加两行代码，可实现多机训练
      
课节2: 基于表格型方法求解RL
      1.强化学习MDP四元组<S,A,P,R>
      2.状态转移与序列决策
      3.Model-free 试错探索
      4.Q表格
      5.Sarsa
      6.Q-learning
      7.on-policy vs off-policy
总结：在基于价值的策略中，注意sarsa和Q_learning之间的算法以及代码之间的区别。sarsa是基于on_policy，而q_learning是基于off_policy。在实战中，个人认为q_learning的收敛性更好。

课节3: 基于神经网络方法求解RL
      DQN:使用神经网络求解RL问题的经典算法（使用神经网络代替Q表格）
      DQN约等于Q-learning +神经网络
      DQN两在创新点：
                  a:使用经验回放，提高样本利用率
                  b.使用同结构的target_network，使算法更平稳
                  
                  
课节4: 基于策略梯度求解RL
      几个知识点的回顾与介绍：
      随机策略：softmax函数
      幕Episode(一轮游戏)
      期望回报
      优化策略函数
      策略梯度
      蒙特卡洛MC与时序差分TD
      Gt /Policy Gradient /Loss
      REINFORCE流程图
      PARL构建智能体agent
      例子：CartPole
      
      
课节5: 连续动作空间上求解RL
      连续动作与离散动作对比：
      DQN与DDPG区别：
      DDPG是DQN的扩展版本，可以扩展到连续控制动作空间。
      PARL构建智能体agent
      
写在最后：
       分享一下在实战中学习到的一点经验。对于固定的网络结构，为了使模型尽快收敛，达到最优，通常需要调整一些参数，如学习率，迭代次数，激活函数等。调整学习率的话，一般先将学习率设置为较大值，然后逐渐减小，变换幅度不能太大。在调整相关参数过程中，要随时观察模型输出的效果，及时调整参数，同时也要注意模型的及时保存（parl 这一点做得很好）。在实战中，建议将模型结果可视化，从而能更好的理解模型，同时观察模型的效果
